```{r setup, include=FALSE, quiet=TRUE}
knitr::opts_chunk$set(echo = TRUE)
library(igraph)
library(ggplot2)
library(caTools)
library(glmnet)
library(mvtnorm)
library(HDCI)
```


```{r}
## data on 8622 households
hh <- read.csv("microfinance_households.csv", row.names="hh")
hh$village <- factor(hh$village)

edges <- read.table("microfinance_edges.txt", colClasses="character")
## edges holds connections between the household ids
hhnet <- graph.edgelist(as.matrix(edges))
hhnet <- as.undirected(hhnet) # two-way connections.

## igraph is all about plotting.  
V(hhnet) ## our 8000+ household vertices
## Each vertex (node) has some attributes, and we can add more.
V(hhnet)$village <- as.character(hh[V(hhnet),'village'])
## we'll color them by village membership
vilcol <- rainbow(nlevels(hh$village))
names(vilcol) <- levels(hh$village)
V(hhnet)$color = vilcol[V(hhnet)$village]
## drop HH labels from plot
V(hhnet)$label=NA

# Graph plots try to force distances proportional to connectivity
# Imagine nodes connected by elastic bands that you are pulling apart
# The graphs can take a very long time, but I've found
# edge.curved=FALSE speeds things up a lot.  Not sure why.

## we'll use induced.subgraph and plot a couple villages 
village1 <- induced.subgraph(hhnet, v=which(V(hhnet)$village=="1"))
village33 <- induced.subgraph(hhnet, v=which(V(hhnet)$village=="33"))

# vertex.size=3 is small.  default is 15
plot(village1, vertex.size=3, edge.curved=FALSE)
plot(village33, vertex.size=3, edge.curved=FALSE)


## match id's
matches <- match(rownames(hh), V(hhnet)$name)

## calculate the 'degree' of each hh: 
##number of commerce/friend/family connections
degree <- degree(hhnet)[matches]
names(degree) <- rownames(hh)
degree[is.na(degree)] <- 0 # unconnected houses, not in our graph
```

```{r}
hh$loan <- factor(hh$loan)

hh$religion <- factor(hh$religion)
levels(hh$religion)

hh$roof <- factor(hh$roof)
levels(hh$roof)

hh$ownership <- factor(hh$ownership)
levels(hh$ownership)

hh$electricity <- factor(hh$electricity)
levels(hh$electricity)

hh$leader <- factor(hh$leader)
levels(hh$leader)

hh1 <- cbind(hh, degree)
```

```{r}
ggplot(data=hh1, aes(x=degree, fill=loan, color=loan)) + geom_histogram(binwidth = 2) + #geom_vline(aes(xintercept=mean(hh$degree)), color="blue", linetype="dashed", size=1) +
   labs(title="Degree histogram plot",x="Degree of Connections", y = "Count")
```

```{r}
log_degree <- log(1+degree)
hh <- cbind(hh, log_degree)
```

#### Significant correlation between number of beds and number of rooms
```{r}
cor(hh$beds, hh$rooms)
cor.test(hh$beds, hh$rooms, method="pearson")
```
```{r}
no_loan_degree_mean <- mean(hh1[hh1$loan==0,'degree'])
print("Mean degree of households with no loan")
print(no_loan_degree_mean)
loan_degree_mean <- mean(hh1[hh1$loan==1,'degree'])
print("Mean degree of households with loan")
print(loan_degree_mean)
print("Mean of degree")
print(mean(degree))
```


```{r}
degree_fequency <- table(degree)
degree_fequency <- as.data.frame(degree_fequency)
degree_fequency
sort(degree_fequency$Freq, decreasing = TRUE)
```

```{r}
str(hh)
summary(hh)
nrow(hh)
colnames(hh)

loan_data_balance <- as.data.frame(table("Loan" = hh$loan))
loan_data_balance$percent <- round(table(hh$loan)/nrow(hh)*100)
loan_data_balance
```

```{r}
ggplot(data=loan_data_balance, aes(x=Loan, y=Freq, fill=Loan)) + geom_bar(stat="identity") + 
   labs(title="Loan datapoints",x="Loan", y = "Frequency")

```

```{r}
split <- sample.split(hh$loan, SplitRatio = 0.8)
train <- subset(hh, split == TRUE)
test <- subset(hh, split == FALSE)
```


```{r}
x <- train[, c(-1, -10)]
y <- train$log_degree
x <- model.matrix(~.-1 + ~.^2, data = x)
model_step1_train <- cv.glmnet(x = x, y = y, alpha = 1)
model_step1_train$lambda.min
model_step1_train$lambda.1se
model_step1_train$lambda
#coef(model_step1_train, model_step1_train$lambda.min)["village1", ]
#model_step1$glmnet.fit
```

#### Model from training dataset
```{r}
plot(model_step1_train)
```

```{r}
x <- hh[, c(-1, -10)]
y <- hh$log_degree
x <- model.matrix(~.-1 + ~.^2, data = x)
model_step1 <- cv.glmnet(x = x, y = y, alpha = 1)
model_step1$lambda.min
model_step1$lambda.1se
model_step1$lambda
#coef(model_step1, model_step1$lambda.min)
```

```{r}
plot(model_step1)
```

```{r}
r_square <- function(y, y_hat) {
  mean_y <- mean(y)
  SST <- sum((y - mean_y)^2)
  SSE <- sum((y - y_hat)^2)
  r_sqr <- 1 - (SSE/SST)
  return(r_sqr)
}
```

#### Out of sample prediction (with r square value)
```{r}
testx <- test[, c(-1,-10)]
testy <- test$log_degree
testx <- model.matrix(~.-1 + ~.^2, data = testx)
d_hat_test <- predict(model_step1_train, testx, s="lambda.1se")
r_square(testy, d_hat_test)
```

#### In sample prediction (with r square value)
```{r}
x <- hh[, c(-1,-10)]
y <- hh$log_degree
x <- model.matrix(~.-1 + ~.^2, data = x)
d_hat <- predict(model_step1, x, s="lambda.1se")
r_square(y, d_hat)
```

```{r}
hh_lasso <- hh
hh_lasso$dhat <- d_hat
x1 <- model.matrix(~.-1, data = hh_lasso[, -1])
y1 <- hh_lasso$loan
penalty <- rep(1, ncol(x1))
penalty[ncol(x1)] <- 0

model_step2 <- cv.glmnet(x = x1, y = y1, nfolds = 5, alpha = 1, family = "binomial", penalty.factor = penalty)

model_step2$lambda.min
model_step2$lambda.1se
model_step2$lambda

print("Treatment  Effect")
coef(model_step2, model_step2$lambda.min)["log_degree", ]
```

```{r}
plot(model_step2)
```

```{r}
x1 <- hh_lasso[, -1]
y1 <- hh_lasso$loan
x1 <- model.matrix(~.-1, data = x1)
loan_y_hat <- predict(model_step2, x1, s="lambda.1se")
```

```{r}
fit <- model_step1$glmnet.fit
tLL <- fit$dev.ratio - deviance(model_step1$glmnet.fit)
k <- fit$df
n <- fit$nobs
AICc <- -tLL + 2*k + 2*k*(k+1)/(n-k-1)
AICc

BIC<-log(n)*k - tLL
BIC

model_step1$cvm

deviance(model_step1$glmnet.fit)/length(y)
```

```{r}
x2 <- model.matrix(~.-1, data = hh[, -1])
y2 <- hh$loan

model_naive <- glmnet(x = x2, y = y2, alpha = 1, family = "binomial")
print("Treatment  Effect")
coef(model_naive, model_naive$lambda.min)["log_degree", ]
coef(model_naive, model_naive$lambda.1se)["log_degree", ]
```

```{r}
plot(model_naive, xvar='lambda')
```

```{r}
#Bootstrap
gamma <-c()
n<-nrow(hh)
for (b in 1:1000){
  ib <- sample(1:n, n, replace = T)
  fit <- glmnet(x2[ib,],y2[ib], alpha = 1, family = 'binomial', lambda = model_step2$lambda.1se)
  gamma <- c(gamma, coef(fit)["log_degree",])
}

```

```{r}
hist(gamma)
ordered_gamma <- sort(gamma)
LCI <- ordered_gamma[5]
UCI <- ordered_gamma[995]
hist(gamma, col="grey40", border="grey90", main="",xlab="gamma")
abline(v=mean(gamma))
polygon(x=c(rep(LCI,2),rep(UCI,2)),
		y=c(0,100,100,0), col=rgb(0,1,0,.2), border=NA)
legend("topright", fill=c(1,"green"), bty="n",
	legend=c("bootstrap (x rand)","95% CI (x given)") )

```


```{r}
#residual bootstrap Lasso and produces confidence intervals for regression coefficients
set.seed(0)
x <- hh_lasso[, -1]
x <- model.matrix(~.-1, data=x)
colnames(x)
y <- hh_lasso$loan
boot100 <- bootLasso(x, y, B = 1000, type.boot = "residual", alpha = .05)
#1000 replications
```

```{r}
#confidence interval for the treatment variable (log_degree)
boot100$interval[,54]
```

```{r}
sum((boot100$interval[1,54]<=beta) & (boot100$interval[2,54]>=beta))
```

